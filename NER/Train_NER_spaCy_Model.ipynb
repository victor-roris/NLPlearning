{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Train NER spaCy Model.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/victor-roris/mediumseries/blob/master/NER/Train_NER_spaCy_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNFEwparVI_A",
        "colab_type": "text"
      },
      "source": [
        "# How train a NER spaCy model \n",
        "\n",
        "In this notebook, we present an example as how train a Named-Entity Recognition model in spaCy to identify specific entities that we labelled previously.\n",
        "\n",
        "For this example, we are going to use the code presented in the entry blog: \n",
        "**Automatic Summarization of Resumes using NER**\n",
        "\n",
        "URL: https://dataturks.com/blog/named-entity-recognition-in-resumes.php\n",
        "\n",
        "GITHUB: https://github.com/DataTurks-Engg/Entity-Recognition-In-Resumes-SpaCy\n",
        "\n",
        "In this example, we are going to train a NER model to identify entities in resumes (skills, name, education, etc.). We are going to use the input data created by `DataTurks` (company who develop the original code). They use a custom format for their input data JSON. We have to transform this custom format to the spaCy valid format."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pkk_H3PgzrRI",
        "colab_type": "text"
      },
      "source": [
        "Import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeS8XllV-Pmy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import random\n",
        "import logging\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from spacy.gold import GoldParse\n",
        "from spacy.scorer import Scorer\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wI7j5osom79G",
        "colab_type": "text"
      },
      "source": [
        "## Input data adaptation\n",
        "\n",
        "The input data (training and test) is formated in JSON with a custom structure defined by `DataTurks`. \n",
        "\n",
        "spaCy use as input format to train the models a list of tuples ```[ (entry1, entry2) ]``` where the tuple is composed by:\n",
        " * 1st part ```(entry1)```: input text\n",
        " * 2nd part ```(entry2)```: list of annotated entities. Each annotated entity is defined by three components:\n",
        "   - 1. start_point\n",
        "   - 2. end_point\n",
        "   - 3. label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpGzQfH1aKiu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from urllib.request import urlopen\n",
        "def get_lines_from_ulr(urlpath):\n",
        "  content = urlopen(urlpath).read().decode(\"utf-8\") \n",
        "  return content.splitlines( )\n",
        "\n",
        "def get_lines_from_file(filepath):\n",
        "  with open(filepath, 'r') as f:\n",
        "    lines = f.readlines()\n",
        "    return lines\n",
        "\n",
        "\n",
        "def convert_dataturks_to_spacy(filepath=None, urlpath=None):\n",
        "    try:\n",
        "        training_data = []\n",
        "        lines=[]\n",
        "        \n",
        "        if filepath :\n",
        "          lines = get_lines_from_file(filepath)\n",
        "        else:\n",
        "          lines = get_lines_from_ulr(urlpath)\n",
        "\n",
        "        for line in lines:\n",
        "\n",
        "            #Each line represents a JSON\n",
        "            data = json.loads(line)\n",
        "\n",
        "            #JSON has two main keys: \n",
        "            #  - content: full text of the resume; \n",
        "            #  - annotation: list of identified parts of text. It is structured:\n",
        "            #      - label: keyword of the type of annotation (ex., skills, email, etc.)\n",
        "            #      - points: span information for the annotation:\n",
        "            #            > start: position of the first token of the annotation\n",
        "            #            > end: position of the last token of the annotation\n",
        "            #            > text: content of the annotation\n",
        "            text = data['content']\n",
        "            entities = []\n",
        "            for annotation in data['annotation']:\n",
        "                #only a single point in text annotation.\n",
        "                point = annotation['points'][0]\n",
        "                labels = annotation['label']\n",
        "                # handle both list of labels or a single label.\n",
        "                if not isinstance(labels, list):\n",
        "                    labels = [labels]\n",
        "\n",
        "                for label in labels:\n",
        "                    #dataturks indices are both inclusive [start, end] but spacy is not [start, end)\n",
        "                    entities.append((point['start'], point['end'] + 1 ,label))\n",
        "                \n",
        "            #Training data for the spacy is a tuple: \n",
        "            #    1. full text\n",
        "            #    2. list of annotated entities [(start_point, end_point, label)]\n",
        "            training_data.append((text, {\"entities\" : entities}))\n",
        "\n",
        "        return training_data\n",
        "    except Exception as e:\n",
        "        logging.exception(\"Unable to process \\n\" + \"error = \" + str(e))\n",
        "        return None\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCbSsgS6oZ2k",
        "colab_type": "text"
      },
      "source": [
        "## Model training\n",
        "\n",
        "We are going to use the input labelled data to train the NER component of the spaCy pipeline to identify the parts of one resume."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7bdeBe1pDHt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\") # create blank Language class"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aj95ZGab0_eG",
        "colab_type": "text"
      },
      "source": [
        "Get the input data in a spaCy valid format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mlPjocqo9dF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "de330eaf-30f9-4532-bfd2-3a0883b7ae79"
      },
      "source": [
        "TRAIN_DATA = convert_dataturks_to_spacy(None, urlpath=\"https://raw.githubusercontent.com/DataTurks-Engg/Entity-Recognition-In-Resumes-SpaCy/master/testdata.json\")\n",
        "print(f'Number of resumes to train : {len(TRAIN_DATA)}')\n",
        "print(f'Ejemplo de input :')\n",
        "TRAIN_DATA[0]"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of resumes to train : 20\n",
            "Ejemplo de input :\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(\"Abhishek Jha\\nApplication Development Associate - Accenture\\n\\nBengaluru, Karnataka - Email me on Indeed: indeed.com/r/Abhishek-Jha/10e7a8cb732bc43a\\n\\n• To work for an organization which provides me the opportunity to improve my skills\\nand knowledge for my individual and company's growth in best possible ways.\\n\\nWilling to relocate to: Bangalore, Karnataka\\n\\nWORK EXPERIENCE\\n\\nApplication Development Associate\\n\\nAccenture -\\n\\nNovember 2017 to Present\\n\\nRole: Currently working on Chat-bot. Developing Backend Oracle PeopleSoft Queries\\nfor the Bot which will be triggered based on given input. Also, Training the bot for different possible\\nutterances (Both positive and negative), which will be given as\\ninput by the user.\\n\\nEDUCATION\\n\\nB.E in Information science and engineering\\n\\nB.v.b college of engineering and technology -  Hubli, Karnataka\\n\\nAugust 2013 to June 2017\\n\\n12th in Mathematics\\n\\nWoodbine modern school\\n\\nApril 2011 to March 2013\\n\\n10th\\n\\nKendriya Vidyalaya\\n\\nApril 2001 to March 2011\\n\\nSKILLS\\n\\nC (Less than 1 year), Database (Less than 1 year), Database Management (Less than 1 year),\\nDatabase Management System (Less than 1 year), Java (Less than 1 year)\\n\\nADDITIONAL INFORMATION\\n\\nTechnical Skills\\n\\nhttps://www.indeed.com/r/Abhishek-Jha/10e7a8cb732bc43a?isid=rex-download&ikw=download-top&co=IN\\n\\n\\n• Programming language: C, C++, Java\\n• Oracle PeopleSoft\\n• Internet Of Things\\n• Machine Learning\\n• Database Management System\\n• Computer Networks\\n• Operating System worked on: Linux, Windows, Mac\\n\\nNon - Technical Skills\\n\\n• Honest and Hard-Working\\n• Tolerant and Flexible to Different Situations\\n• Polite and Calm\\n• Team-Player\",\n",
              " {'entities': [(1295, 1622, 'Skills'),\n",
              "   (993, 1154, 'Skills'),\n",
              "   (939, 957, 'College Name'),\n",
              "   (883, 905, 'College Name'),\n",
              "   (856, 861, 'Graduation Year'),\n",
              "   (771, 814, 'College Name'),\n",
              "   (727, 770, 'Designation'),\n",
              "   (407, 416, 'Companies worked at'),\n",
              "   (372, 405, 'Designation'),\n",
              "   (95, 146, 'Email Address'),\n",
              "   (60, 69, 'Location'),\n",
              "   (49, 58, 'Companies worked at'),\n",
              "   (13, 46, 'Designation'),\n",
              "   (0, 12, 'Name')]})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KRDEV1CqQ68",
        "colab_type": "text"
      },
      "source": [
        "Get or create a component in the pipeline to the Named-Entity Recognition (NER)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rolbXRVfph2_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  # create the built-in pipeline components and add them to the pipeline\n",
        "  # nlp.create_pipe works for built-ins that are registered with spaCy\n",
        "  if 'ner' not in nlp.pipe_names:\n",
        "      ner = nlp.create_pipe('ner')\n",
        "      nlp.add_pipe(ner, last=True)\n",
        "  else:\n",
        "    ner = nlp.get_pipe('ner')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIplfTnvqXib",
        "colab_type": "text"
      },
      "source": [
        "Extract from the input data the considered labels and add them to the NER label collection "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKIL9Ag6poko",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# add labels\n",
        "for _, annotations in TRAIN_DATA:\n",
        "  for ent in annotations.get('entities'):\n",
        "    ner.add_label(ent[2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UF8WCGsqrT-",
        "colab_type": "text"
      },
      "source": [
        "**Training the NER model with the new labels**\n",
        "\n",
        "Disable the other components in the pipeline during training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCxDPtiDql37",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "bb804c86-844b-49c1-933c-6a6ec774236e"
      },
      "source": [
        "# get names of other pipes to disable them during training\n",
        "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
        "with nlp.disable_pipes(*other_pipes):  # only train NER\n",
        "    optimizer = nlp.begin_training()\n",
        "    for itn in range(10):\n",
        "        print(\"Statring iteration \" + str(itn))\n",
        "        random.shuffle(TRAIN_DATA)\n",
        "        losses = {}\n",
        "        for text, annotations in TRAIN_DATA:\n",
        "            nlp.update(\n",
        "                [text],  # batch of texts\n",
        "                [annotations],  # batch of annotations\n",
        "                drop=0.2,  # dropout - make it harder to memorise data\n",
        "                sgd=optimizer,  # callable to update weights\n",
        "                losses=losses)\n",
        "        print(losses)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Statring iteration 0\n",
            "{'ner': 10714.913392737508}\n",
            "Statring iteration 1\n",
            "{'ner': 9426.441921290443}\n",
            "Statring iteration 2\n",
            "{'ner': 8264.782624161802}\n",
            "Statring iteration 3\n",
            "{'ner': 7602.219878260046}\n",
            "Statring iteration 4\n",
            "{'ner': 7774.348719151691}\n",
            "Statring iteration 5\n",
            "{'ner': 7243.808744228678}\n",
            "Statring iteration 6\n",
            "{'ner': 6951.896825157804}\n",
            "Statring iteration 7\n",
            "{'ner': 7294.837433060631}\n",
            "Statring iteration 8\n",
            "{'ner': 6837.749791778624}\n",
            "Statring iteration 9\n",
            "{'ner': 6894.473289556801}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_A4B-G61KBs",
        "colab_type": "text"
      },
      "source": [
        "## Model evaluation\n",
        "\n",
        "We evaluate the trained NER model using the input test data provided for `DataTurks`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Nh2y8MTaQg2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "outputId": "19cb5e67-3379-404c-a90f-81e5ab793b83"
      },
      "source": [
        "#test the model and evaluate it\n",
        "examples = convert_dataturks_to_spacy(None, \"https://raw.githubusercontent.com/DataTurks-Engg/Entity-Recognition-In-Resumes-SpaCy/master/testdata.json\")\n",
        "tp=0\n",
        "tr=0\n",
        "tf=0\n",
        "\n",
        "ta=0\n",
        "c=0        \n",
        "for text,annot in examples:\n",
        "\n",
        "    f=open(\"resume\"+str(c)+\".txt\",\"w\")\n",
        "    doc_to_test=nlp(text)\n",
        "    d={}\n",
        "    for ent in doc_to_test.ents:\n",
        "        d[ent.label_]=[]\n",
        "    for ent in doc_to_test.ents:\n",
        "        d[ent.label_].append(ent.text)\n",
        "\n",
        "    for i in set(d.keys()):\n",
        "\n",
        "        f.write(\"\\n\\n\")\n",
        "        f.write(i +\":\"+\"\\n\")\n",
        "        for j in set(d[i]):\n",
        "            f.write(j.replace('\\n','')+\"\\n\")\n",
        "    d={}\n",
        "    for ent in doc_to_test.ents:\n",
        "        d[ent.label_]=[0,0,0,0,0,0]\n",
        "    for ent in doc_to_test.ents:\n",
        "        doc_gold_text= nlp.make_doc(text)\n",
        "        gold = GoldParse(doc_gold_text, entities=annot.get(\"entities\"))\n",
        "        y_true = [ent.label_ if ent.label_ in x else 'Not '+ent.label_ for x in gold.ner]\n",
        "        y_pred = [x.ent_type_ if x.ent_type_ ==ent.label_ else 'Not '+ent.label_ for x in doc_to_test]  \n",
        "        if(d[ent.label_][0]==0):\n",
        "            #f.write(\"For Entity \"+ent.label_+\"\\n\")   \n",
        "            #f.write(classification_report(y_true, y_pred)+\"\\n\")\n",
        "            (p,r,f,s)= precision_recall_fscore_support(y_true,y_pred,average='weighted')\n",
        "            a=accuracy_score(y_true,y_pred)\n",
        "            d[ent.label_][0]=1\n",
        "            d[ent.label_][1]+=p\n",
        "            d[ent.label_][2]+=r\n",
        "            d[ent.label_][3]+=f\n",
        "            d[ent.label_][4]+=a\n",
        "            d[ent.label_][5]+=1\n",
        "    c+=1\n",
        "for i in d:\n",
        "    print(\"\\n For Entity \"+i+\"\\n\")\n",
        "    print(\"Accuracy : \"+str((d[i][4]/d[i][5])*100)+\"%\")\n",
        "    print(\"Precision : \"+str(d[i][1]/d[i][5]))\n",
        "    print(\"Recall : \"+str(d[i][2]/d[i][5]))\n",
        "    print(\"F-score : \"+str(d[i][3]/d[i][5]))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
            "  'recall', 'true', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " For Entity Name\n",
            "\n",
            "Accuracy : 99.83805668016194%\n",
            "Precision : 0.9983831936194594\n",
            "Recall : 0.9983805668016195\n",
            "F-score : 0.9981113185060555\n",
            "\n",
            " For Entity Location\n",
            "\n",
            "Accuracy : 99.10931174089069%\n",
            "Precision : 0.9838647235218079\n",
            "Recall : 0.9910931174089069\n",
            "F-score : 0.987465692416357\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YD8WfB26rlG4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yby_SnDXx0j3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}