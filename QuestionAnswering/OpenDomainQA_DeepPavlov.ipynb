{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "OpenDomainQA-DeepPavlov.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOfRK5A5HrKVotmzL+eO2de",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/victor-roris/NLPlearning/blob/master/QuestionAnswering/OpenDomainQA_DeepPavlov.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SB5vfD4gRqC"
      },
      "source": [
        "## DeepPavlov - Open Domain Question Answering\n",
        "\n",
        "Adapted from: [link](https://colab.research.google.com/github/deepmipt/dp_notebooks/blob/master/DP_ODQA.ipynb)\n",
        "\n",
        "The architecture of the DeepPavlov ODQA skill is modular and consists of two components: a **ranker** and a **reader**.\n",
        "\n",
        "In order to answer any question, the **ranker** first retrieves a few relevant articles from the article collection, and then the **reader** scans them carefully to identify the answer. The **ranker** is based on DrQA [1] proposed by Facebook Research. Specifically, the DrQA approach uses unigram-bigram hashing and TF-IDF matching designed to efficiently return a subset of relevant articles based on a question. \n",
        "\n",
        "The **reader** is based on R-NET [2] proposed by Microsoft Research Asia and its implementation by Wenxuan Zhou. The R-NET architecture is an end-to-end neural network model that aims to answer questions based on a given article. R-NET first matches the question and the article via gated attention-based recurrent networks to obtain a question-aware article representation. Then the self-matching attention mechanism refines the representation by matching the article against itself, which effectively encodes information from the whole article. Finally, the pointer networks locate the positions of answers in the article. \n",
        "\n",
        "The scheme below shows DeepPavlov ODQA system architecture.\n",
        "\n",
        "<img src=\"https://github.com/deepmipt/dp_notebooks/blob/master/odqa.png?raw=1\">\n",
        "\n",
        "<center>Picture 1. The DeepPavlov-based ODQA system architecture</center>\n",
        "\n",
        "\n",
        "DeepPavlovâ€™s ODQA system has two Wikipedia-based models. The first one is based on the English Wikipedia dump from 2018-02-11 (5,180,368 articles) and the second one is based on the Russian Wikipedia dump from 2018-04-01 (1,463,888 articles).\n",
        "\n",
        "[1] [Chen, Danqi, et al. \"Reading wikipedia to answer open-domain questions.\" arXiv preprint arXiv:1704.00051 (2017)](https://arxiv.org/pdf/1704.00051.pdf)\n",
        "\n",
        "[2] [R-NET: Machine reading comprehension with self-matching networks](https://www.microsoft.com/en-us/research/wp-content/uploads/2017/05/r-net.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdXeMGsjg8mq"
      },
      "source": [
        "# Model Requirements\n",
        "\n",
        "Install DeepPavlov and all the model's requirements."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vH7hAfQJhC4F"
      },
      "source": [
        "!pip install -q deeppavlov\n",
        "!python -m deeppavlov install en_odqa_infer_wiki"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnTNskGhidBE"
      },
      "source": [
        "# Model Description\n",
        "\n",
        "The architecture of the ODQA skill is modular and consists of two components, a **ranker** and a **reader**. In order to answer any question, the **ranker** first retrieves **top_n** relevant articles from the document collection, and then the **reader** scans them carefully to identify the answer. The detailed description of the ODQA models can be found in the [DeepPavlov documentation](http://docs.deeppavlov.ai/en/0.1.6/skills/odqa.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aICbhy4eRJb0"
      },
      "source": [
        "# %load: Load code into the current frontend\n",
        "%load https://github.com/deepmipt/DeepPavlov/blob/0.1.6/deeppavlov/configs/odqa/en_odqa_infer_wiki.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQg-gPXIjFYZ",
        "outputId": "82bb5d60-449f-4ab4-a3da-6c3c815ef15e"
      },
      "source": [
        "# Visualize what just load in the frontend\n",
        "import urllib.request, json \n",
        "with urllib.request.urlopen(\"https://raw.githubusercontent.com/deepmipt/DeepPavlov/0.1.6/deeppavlov/configs/odqa/en_odqa_infer_wiki.json\") as url:\n",
        "    data = json.loads(url.read().decode())\n",
        "    print(json.dumps(data, indent=4, sort_keys=True))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "    \"chainer\": {\n",
            "        \"in\": [\n",
            "            \"question_raw\"\n",
            "        ],\n",
            "        \"out\": [\n",
            "            \"best_answer\"\n",
            "        ],\n",
            "        \"pipe\": [\n",
            "            {\n",
            "                \"config_path\": \"{CONFIGS_PATH}/doc_retrieval/en_ranker_tfidf_wiki.json\",\n",
            "                \"in\": [\n",
            "                    \"question_raw\"\n",
            "                ],\n",
            "                \"out\": [\n",
            "                    \"tfidf_doc_ids\"\n",
            "                ]\n",
            "            },\n",
            "            {\n",
            "                \"class_name\": \"wiki_sqlite_vocab\",\n",
            "                \"in\": [\n",
            "                    \"tfidf_doc_ids\"\n",
            "                ],\n",
            "                \"join_docs\": false,\n",
            "                \"load_path\": \"{DOWNLOADS_PATH}/odqa/enwiki.db\",\n",
            "                \"out\": [\n",
            "                    \"tfidf_doc_text\"\n",
            "                ],\n",
            "                \"shuffle\": false\n",
            "            },\n",
            "            {\n",
            "                \"class_name\": \"document_chunker\",\n",
            "                \"flatten_result\": true,\n",
            "                \"in\": [\n",
            "                    \"tfidf_doc_text\"\n",
            "                ],\n",
            "                \"out\": [\n",
            "                    \"chunks\"\n",
            "                ],\n",
            "                \"paragraphs\": true\n",
            "            },\n",
            "            {\n",
            "                \"class_name\": \"string_multiplier\",\n",
            "                \"in\": [\n",
            "                    \"question_raw\",\n",
            "                    \"chunks\"\n",
            "                ],\n",
            "                \"out\": [\n",
            "                    \"questions\"\n",
            "                ]\n",
            "            },\n",
            "            {\n",
            "                \"batch_size\": 10,\n",
            "                \"class_name\": \"logit_ranker\",\n",
            "                \"in\": [\n",
            "                    \"chunks\",\n",
            "                    \"questions\"\n",
            "                ],\n",
            "                \"out\": [\n",
            "                    \"best_answer\",\n",
            "                    \"best_answer_score\"\n",
            "                ],\n",
            "                \"sort_noans\": true,\n",
            "                \"squad_model\": {\n",
            "                    \"config_path\": \"{CONFIGS_PATH}/squad/multi_squad_noans_infer.json\"\n",
            "                }\n",
            "            }\n",
            "        ]\n",
            "    },\n",
            "    \"metadata\": {\n",
            "        \"download\": [],\n",
            "        \"labels\": {\n",
            "            \"server_utils\": \"ODQA\"\n",
            "        },\n",
            "        \"requirements\": [\n",
            "            \"{DEEPPAVLOV_PATH}/requirements/tf.txt\",\n",
            "            \"{DEEPPAVLOV_PATH}/requirements/spacy.txt\",\n",
            "            \"{DEEPPAVLOV_PATH}/requirements/en_core_web_sm.txt\"\n",
            "        ],\n",
            "        \"variables\": {\n",
            "            \"CONFIGS_PATH\": \"{DEEPPAVLOV_PATH}/configs\",\n",
            "            \"DOWNLOADS_PATH\": \"{ROOT_PATH}/downloads\",\n",
            "            \"MODELS_PATH\": \"{ROOT_PATH}/models\",\n",
            "            \"ROOT_PATH\": \"~/.deeppavlov\"\n",
            "        }\n",
            "    }\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rouZRP85lq5u"
      },
      "source": [
        "## Training the model\n",
        "\n",
        "You can train a model by running the framework with **train** parameter, wherein the model will be trained on the document collection defined in the **dataset_reader** section of the configuration file. The **dataset_reader** section of the rankerâ€™s configuration defines the source of the articles. The source can be of the following **dataset_format-**:\n",
        "\n",
        "wiki â€” the Wikipedia dump,\n",
        "txt â€” the path to the separated text files,\n",
        "json â€” JSON files, which should be formatted as a list with dicts that contain the *title* and *doc* keywords.\n",
        "\n",
        "\n",
        "* *wiki* - The Wikipedia dump\n",
        "* *txt* - each document in separate txt file\n",
        "* *json* - JSON files should be formatted as list with dicts which contain 'title' and 'doc' keywords.\n",
        "\n",
        "As a training corpus, I will use the PloS sentence corpus. It consists of 300 computational biology articles, each of them stored in a separate *txt* file. For simplicity, we will use the same configuration files that is used for the Wikipedia-based ODQA system; however, we strongly encourage you to create custom configuration files for your own models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSoAIn9qV95-"
      },
      "source": [
        "!wget -q http://archive.ics.uci.edu/ml/machine-learning-databases/00311/SentenceCorpus.zip\n",
        "!unzip SentenceCorpus.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4ddAJYERNJP",
        "outputId": "a4e501cc-afbb-4437-ce61-ce716d4ea975"
      },
      "source": [
        "!ls SentenceCorpus"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Instructions_for_SentenceAnnotation.pdf  README\t\t     word_lists\n",
            "labeled_articles\t\t\t unlabeled_articles\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkIRKgHpmZj-"
      },
      "source": [
        "In order to fit a model on new data, first, change the data_path parameter of the dataset_reader section. Then change the dataset_format to txt. Finally, train the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Anvf7piRRklQ",
        "outputId": "29d6d235-3da3-4a46-973a-e9ee9e780e67"
      },
      "source": [
        "from deeppavlov import configs\n",
        "from deeppavlov.core.common.file import read_json\n",
        "from deeppavlov import configs, train_model\n",
        "\n",
        "model_config = read_json(configs.doc_retrieval.en_ranker_tfidf_wiki)\n",
        "model_config[\"dataset_reader\"][\"data_path\"] = \"/content/SentenceCorpus/unlabeled_articles/plos_unlabeled\"\n",
        "model_config[\"dataset_reader\"][\"dataset_format\"] = \"txt\"\n",
        "doc_retrieval = train_model(model_config)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-05-26 12:15:19.329 INFO in 'deeppavlov.dataset_readers.odqa_reader'['odqa_reader'] at line 57: Reading files...\n",
            "2021-05-26 12:15:19.335 INFO in 'deeppavlov.dataset_readers.odqa_reader'['odqa_reader'] at line 134: Building the database...\n",
            "  0%|          | 0/300 [00:00<?, ?it/s]\n",
            "300it [00:00, 4450.32it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 3018.72it/s]\n",
            "2021-05-26 12:15:19.595 INFO in 'deeppavlov.dataset_iterators.sqlite_iterator'['sqlite_iterator'] at line 57: Connecting to database, path: /root/.deeppavlov/downloads/odqa/enwiki.db\n",
            "2021-05-26 12:15:19.598 INFO in 'deeppavlov.dataset_iterators.sqlite_iterator'['sqlite_iterator'] at line 112: SQLite iterator: The size of the database is 300 documents\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package perluniprops to /root/nltk_data...\n",
            "[nltk_data]   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data] Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "2021-05-26 12:15:23.403 INFO in 'deeppavlov.models.vectorizers.hashing_tfidf_vectorizer'['hashing_tfidf_vectorizer'] at line 153: Tokenizing batch...\n",
            "2021-05-26 12:16:01.340 INFO in 'deeppavlov.models.vectorizers.hashing_tfidf_vectorizer'['hashing_tfidf_vectorizer'] at line 155: Counting hash...\n",
            "2021-05-26 12:16:01.541 INFO in 'deeppavlov.models.vectorizers.hashing_tfidf_vectorizer'['hashing_tfidf_vectorizer'] at line 214: Saving tfidf matrix to /root/.deeppavlov/models/odqa/enwiki_tfidf_matrix.npz\n",
            "2021-05-26 12:16:04.129 INFO in 'deeppavlov.models.vectorizers.hashing_tfidf_vectorizer'['hashing_tfidf_vectorizer'] at line 264: Loading tfidf matrix from /root/.deeppavlov/models/odqa/enwiki_tfidf_matrix.npz\n",
            "2021-05-26 12:16:04.983 INFO in 'deeppavlov.models.vectorizers.hashing_tfidf_vectorizer'['hashing_tfidf_vectorizer'] at line 264: Loading tfidf matrix from /root/.deeppavlov/models/odqa/enwiki_tfidf_matrix.npz\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtqODIYLmqAk"
      },
      "source": [
        "Examine the ranker output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCK9i--3Ru-5",
        "outputId": "06241bd5-a5ba-4fac-a08d-0fa01ae946ce"
      },
      "source": [
        "query_term = 'cerebellum'\n",
        "print(f\"Ranker of documents in the PloS sentence corpus for the term '{query_term}'\")\n",
        "for doc_title in doc_retrieval([query_term])[0]:\n",
        "  print(f\"\\t - {doc_title}\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ranker of documents in the PloS sentence corpus for the term 'cerebellum'\n",
            "\t - 46.txt\n",
            "\t - 453.txt\n",
            "\t - 490.txt\n",
            "\t - 485.txt\n",
            "\t - 484.txt\n",
            "\t - 478.txt\n",
            "\t - 470.txt\n",
            "\t - 466.txt\n",
            "\t - 499.txt\n",
            "\t - 430.txt\n",
            "\t - 437.txt\n",
            "\t - 429.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyW8-CX0nHT9"
      },
      "source": [
        "Everything is done to run the ODQA component, make sure that the `download = False` otherwise the pretrained Wikipedia dump will overwrite your model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f47HSRL0m8kY"
      },
      "source": [
        "from deeppavlov import configs\n",
        "from deeppavlov.core.commands.infer import build_model"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYuHXigdnSSG"
      },
      "source": [
        "# Download all the SQuAD models\n",
        "squad = build_model(configs.squad.multi_squad_noans_infer, download = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFtO__nbnUA0",
        "outputId": "5b9f115d-4871-4c74-9b01-a59983f85e2e"
      },
      "source": [
        "# Do not download the ODQA models, we've just trained it\n",
        "odqa = build_model(configs.odqa.en_odqa_infer_wiki, download = False)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-05-26 12:23:17.51 INFO in 'deeppavlov.models.vectorizers.hashing_tfidf_vectorizer'['hashing_tfidf_vectorizer'] at line 264: Loading tfidf matrix from /root/.deeppavlov/models/odqa/enwiki_tfidf_matrix.npz\n",
            "2021-05-26 12:23:17.538 INFO in 'deeppavlov.dataset_iterators.sqlite_iterator'['sqlite_iterator'] at line 57: Connecting to database, path: /root/.deeppavlov/downloads/odqa/enwiki.db\n",
            "2021-05-26 12:23:17.540 INFO in 'deeppavlov.dataset_iterators.sqlite_iterator'['sqlite_iterator'] at line 112: SQLite iterator: The size of the database is 300 documents\n",
            "2021-05-26 12:23:17.550 INFO in 'deeppavlov.models.preprocessors.squad_preprocessor'['squad_preprocessor'] at line 310: SquadVocabEmbedder: loading saved tokens vocab from /root/.deeppavlov/models/multi_squad_model_noans/emb/vocab_embedder.pckl\n",
            "2021-05-26 12:23:18.218 INFO in 'deeppavlov.models.preprocessors.squad_preprocessor'['squad_preprocessor'] at line 310: SquadVocabEmbedder: loading saved chars vocab from /root/.deeppavlov/models/multi_squad_model_noans/emb/char_vocab_embedder.pckl\n",
            "2021-05-26 12:23:18.501 INFO in 'deeppavlov.core.layers.tf_layers'['tf_layers'] at line 619: \n",
            "Warning! tf.contrib.cudnn_rnn.CudnnCompatibleGRUCell is used. It is okay for inference mode, but if you train your model with this cell it could NOT be used with tf.contrib.cudnn_rnn.CudnnGRUCell later. \n",
            "2021-05-26 12:23:18.624 INFO in 'deeppavlov.core.layers.tf_layers'['tf_layers'] at line 619: \n",
            "Warning! tf.contrib.cudnn_rnn.CudnnCompatibleGRUCell is used. It is okay for inference mode, but if you train your model with this cell it could NOT be used with tf.contrib.cudnn_rnn.CudnnGRUCell later. \n",
            "2021-05-26 12:23:18.754 INFO in 'deeppavlov.core.layers.tf_layers'['tf_layers'] at line 619: \n",
            "Warning! tf.contrib.cudnn_rnn.CudnnCompatibleGRUCell is used. It is okay for inference mode, but if you train your model with this cell it could NOT be used with tf.contrib.cudnn_rnn.CudnnGRUCell later. \n",
            "2021-05-26 12:23:18.841 INFO in 'deeppavlov.core.layers.tf_layers'['tf_layers'] at line 619: \n",
            "Warning! tf.contrib.cudnn_rnn.CudnnCompatibleGRUCell is used. It is okay for inference mode, but if you train your model with this cell it could NOT be used with tf.contrib.cudnn_rnn.CudnnGRUCell later. \n",
            "2021-05-26 12:23:29.481 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 51: [loading model from /root/.deeppavlov/models/multi_squad_model_noans/model]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /root/.deeppavlov/models/multi_squad_model_noans/model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NIp1wT-ingGd",
        "outputId": "9a279d1c-3095-4d45-fc2f-256ea365b509"
      },
      "source": [
        "# Ask\n",
        "answers = odqa([\"what is tuberculosis?\", \"what is a gene?\"])\n",
        "answers"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-05-26 12:26:10.503 WARNING in 'deeppavlov.models.doc_retrieval.logit_ranker'['logit_ranker'] at line 74: you didn't pass tfidf_doc_ids as input in logit_ranker config so batch_best_answers_doc_ids can't be compute\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['a disease for which a new drug is desperately needed', ''],\n",
              " [0.9975703954696655, 0.7835770845413208],\n",
              " [314, -1]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzOLnrl2o9a4",
        "outputId": "863ec859-253d-405b-d785-d3d307cf9ad8"
      },
      "source": [
        "answers = odqa([\"what is a cycling transcription network?\"])\n",
        "answer = answers[0]\n",
        "print(f\"Answer: {answers[0][0]}\")\n",
        "print(f\"Score: {answers[1][0]}\")\n",
        "print(f\"Id: {answers[2][0]}\")"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-05-26 12:33:38.623 WARNING in 'deeppavlov.models.doc_retrieval.logit_ranker'['logit_ranker'] at line 74: you didn't pass tfidf_doc_ids as input in logit_ranker config so batch_best_answers_doc_ids can't be compute\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Answer: Eukaryotes\n",
            "Score: 0.9929509162902832\n",
            "Id: 2402\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kszr7UZqlI5m"
      },
      "source": [
        "## Interacting with the DeepPavlovâ€™s PreTrained models\n",
        "\n",
        "The DeepPavlov ODQA system has two Wikipedia-based models. The English Wikipedia model requires 35 GB of local storage, whereas the Russian version takes up about 20 GB. The Wikipedia dumps can be rebuilt by steps described in the [documentation](http://docs.deeppavlov.ai/en/0.1.6/components/tfidf_ranking.html#available-data-and-pretrained-models). Both models require about 24 GB of RAM. It is possible to run them on a 16 GB machine, but the swap size should be at least 8 GB.\n",
        "\n",
        "**As it was mentioned, the Wikipedia-based models have significant storage and RAM requirements, therefore it's impossible to interact with them on Colab, however you can do so localy (of course when the requirements are satisfied). **\n",
        "\n",
        "Alternatively, you can check out our [demo](http://demo.ipavlov.ai/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wj5V6jn8qzuu"
      },
      "source": [
        "## Useful links\n",
        "\n",
        "[DeepPavlov repository](https://github.com/deepmipt/DeepPavlov)\n",
        "\n",
        "[DeepPavlov demo page](https://demo.ipavlov.ai)\n",
        "\n",
        "[DeepPavlov documentation](https://docs.deeppavlov.ai)"
      ]
    }
  ]
}