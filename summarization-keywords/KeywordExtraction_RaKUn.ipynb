{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KeywordExtraction-RaKUn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMkVubPGTXw5FgP2R7+Zbst",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/victor-roris/NLPlearning/blob/master/summarization-keywords/KeywordExtraction_RaKUn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srbS2qjYyjyD"
      },
      "source": [
        "# RaKUn\n",
        "\n",
        "This keyword detection algorithm exploits graph-based language representations for efficient denoising and keyword detection. Key ideas of RaKUn:\n",
        "1. Transform texts to graphs\n",
        "2. Prune graphs based on token similarity (meta vertex introduction)\n",
        "3. Rank nodes -> keywords\n",
        "\n",
        "\n",
        "PAPER: https://arxiv.org/pdf/1907.06458v3.pdf\n",
        "\n",
        "GitHub: https://github.com/SkBlaz/rakun\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MHl1YYgCK0-"
      },
      "source": [
        "## Install"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUwdZDqJyj_T"
      },
      "source": [
        "!pip install mrakun"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zt0Y9U6BSOH8"
      },
      "source": [
        "## Document of study\n",
        "\n",
        "We are going to apply keyword Extraction algorithms in a specific text. The idea is use always the same content to study the different results. At same time, it is important know the document to evaluate if the results are valid or not. \n",
        "\n",
        "To reach this goal, we are going to use an scientific article text. Furthermore, we remove the abstract and the keywords of the content.\n",
        "\n",
        "The authors labelled the document with the abstract and keywords:\n",
        "\n",
        "* **Abstract**: The provision of comprehensive support for traceability and control is a raising demand in some environments such as the eHealth domain where processes can be of critical importance. This paper provides a detailed and thoughtful description of a holistic platform for the characterization and control of processes in the frame of the HACCP context. Traceability features are fully integrated in the model along with support for services concerned with information for the platform users. These features are provided using already tested technologies (RESTful models, QR Codes) and low cost devices (regular smartphones).\n",
        "\n",
        "* **Keywords**: traceability, eHealth, software platform, mobile environments\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuF9hHkNOHdJ"
      },
      "source": [
        "Download the text file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOzgy9l-INWF"
      },
      "source": [
        "!wget -O article.txt https://www.dropbox.com/s/1mz0ociy6ipz67q/victor_roris-worldcist2016.txt?dl=1 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Hf8Qp46Ujs_"
      },
      "source": [
        "Read the content"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObFdiNydJMJ1"
      },
      "source": [
        "# Open a file: file\n",
        "content = \"\"\n",
        "with open('article.txt',mode='r') as file:\n",
        "  content = file.read()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KPCRrisJPog",
        "outputId": "93bbe86c-1c77-4836-b949-6306f80c7d01"
      },
      "source": [
        "print(f\"Number of words : {len(content.split())}\")\n",
        "print(\"First lines:\")\n",
        "for line in content.split(\"\\n\")[0:3]:\n",
        "  print(line)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of words : 3830\n",
            "First lines:\n",
            "﻿________________\n",
            "A telematic based approach towards the normalization of clinical praxis\n",
            "Víctor M. Alonso Rorís1, Juan M. Santos Gago1, Luis Álvarez Sabucedo1, \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_8O2a1V3bs8"
      },
      "source": [
        "## Apply RaKUn\n",
        "\n",
        "Hyperparameter description: https://github.com/SkBlaz/rakun#hyperparameter-explanation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahi2FH3akNhU",
        "outputId": "b3cba5b6-6324-487d-b112-ed4d3ef5df46"
      },
      "source": [
        "from mrakun import RakunDetector\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "hyperparameters = {\"distance_threshold\":2,\n",
        "                   \"distance_method\": \"editdistance\",\n",
        "                   \"num_keywords\" : 10,\n",
        "                   \"pair_diff_length\":2,\n",
        "                   \"stopwords\" : stopwords.words('english'),\n",
        "                   \"bigram_count_threshold\":3,\n",
        "                   \"num_tokens\":[1,2],\n",
        "\t\t   \"max_similar\" : 3, ## n most similar can show up n times\n",
        "\t\t   \"max_occurrence\" : 3} ## maximum frequency overall\n",
        "\n",
        "keyword_detector = RakunDetector(hyperparameters)\n",
        "keywords = keyword_detector.find_keywords(content, input_type = \"text\")\n",
        "\n",
        "print()\n",
        "keywords\n",
        "# keyword_detector.visualize_network()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "19-May-21 12:23:47 - Initiated a keyword detector instance.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "19-May-21 12:23:51 - Number of nodes reduced from 877 to 800\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('control', 0.15019487151274621),\n",
              " ('system', 0.11831072413827162),\n",
              " ('information', 0.1072080768975499),\n",
              " ('control and traceability', 0.09631130380535535),\n",
              " ('control and traceability', 0.09631130380535535),\n",
              " ('control and traceability', 0.09631130380535535),\n",
              " ('application', 0.07461086465298247),\n",
              " ('operations', 0.06307873482228536),\n",
              " ('description', 0.06269849996528838),\n",
              " ('procedures', 0.055083288992466485)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kbj6wkUXokoN"
      },
      "source": [
        "### Use fasttext\n",
        "\n",
        "Using RaKUn with fasttext requires pretrained emmbeding model. Download .bin file from https://github.com/facebookresearch/fastText/blob/master/docs/pretrained-vectors.md for chosen language and save it.\n",
        "\n",
        "```python\n",
        "from mrakun import RakunDetector\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "hyperparameters = {\"distance_threshold\":0.2,\n",
        "                   \"distance_method\": \"fasttext\",\n",
        "                   \"pretrained_embedding_path\": '../pretrained_models/fasttext/wiki.en.bin', #change path accordingly\n",
        "                   \"num_keywords\" : 10,\n",
        "                   \"pair_diff_length\":2,\n",
        "                   \"stopwords\" : stopwords.words('english'),\n",
        "                   \"bigram_count_threshold\":2,\n",
        "                   \"num_tokens\":[1]}\n",
        "\n",
        "keyword_detector = RakunDetector(hyperparameters)\n",
        "example_data = \"./datasets/wiki20/docsutf8/7183.txt\"\n",
        "keywords = keyword_detector.find_keywords(example_data)\n",
        "print(keywords)\n",
        "\n",
        "keyword_detector.verbose = False\n",
        "```"
      ]
    }
  ]
}