{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KeywordExtraction-KEX.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP8Ig1wBH2ozTxeEtiBH99d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/victor-roris/NLPlearning/blob/master/summarization-keywords/KeywordExtraction_KEX.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srbS2qjYyjyD"
      },
      "source": [
        "# KEX\n",
        "\n",
        "Kex is a python library for unsurpervised keyword extractions, supporting the following features:\n",
        "\n",
        "- Easy interface for keyword extraction with a variety of algorithms\n",
        "- Quick benchmarking over 15 English public datasets\n",
        "- Custom keyword extractor implementation support\n",
        "\n",
        "PAPER: https://arxiv.org/pdf/2104.08028v1.pdf\n",
        "\n",
        "GitHub: https://github.com/asahi417/kex\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MHl1YYgCK0-"
      },
      "source": [
        "## Install"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUwdZDqJyj_T",
        "outputId": "541cadb7-6fce-43b4-b5c6-1e069d9f4537"
      },
      "source": [
        "!pip install kex"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kex in /usr/local/lib/python3.7/dist-packages (2.0.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kex) (2.23.0)\n",
            "Requirement already satisfied: segtok in /usr/local/lib/python3.7/dist-packages (from kex) (1.5.10)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from kex) (2.5.1)\n",
            "Requirement already satisfied: numpy>=1.16.1 in /usr/local/lib/python3.7/dist-packages (from kex) (1.19.5)\n",
            "Requirement already satisfied: nltk==3.5 in /usr/local/lib/python3.7/dist-packages (from kex) (3.5)\n",
            "Requirement already satisfied: gensim<3.5.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from kex) (3.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->kex) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kex) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kex) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->kex) (1.24.3)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from segtok->kex) (2019.12.20)\n",
            "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx->kex) (4.4.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk==3.5->kex) (8.0.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk==3.5->kex) (1.0.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk==3.5->kex) (4.41.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim<3.5.0,>=3.4.0->kex) (5.0.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim<3.5.0,>=3.4.0->kex) (1.4.1)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim<3.5.0,>=3.4.0->kex) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zt0Y9U6BSOH8"
      },
      "source": [
        "## Document of study\n",
        "\n",
        "We are going to apply keyword Extraction algorithms in a specific text. The idea is use always the same content to study the different results. At same time, it is important know the document to evaluate if the results are valid or not. \n",
        "\n",
        "To reach this goal, we are going to use an scientific article text. Furthermore, we remove the abstract and the keywords of the content.\n",
        "\n",
        "The authors labelled the document with the abstract and keywords:\n",
        "\n",
        "* **Abstract**: The provision of comprehensive support for traceability and control is a raising demand in some environments such as the eHealth domain where processes can be of critical importance. This paper provides a detailed and thoughtful description of a holistic platform for the characterization and control of processes in the frame of the HACCP context. Traceability features are fully integrated in the model along with support for services concerned with information for the platform users. These features are provided using already tested technologies (RESTful models, QR Codes) and low cost devices (regular smartphones).\n",
        "\n",
        "* **Keywords**: traceability, eHealth, software platform, mobile environments\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuF9hHkNOHdJ"
      },
      "source": [
        "Download the text file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOzgy9l-INWF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6f3a59b-ec54-4e96-b685-0c04a4630724"
      },
      "source": [
        "!wget -O article.txt https://www.dropbox.com/s/1mz0ociy6ipz67q/victor_roris-worldcist2016.txt?dl=1 "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-19 11:44:58--  https://www.dropbox.com/s/1mz0ociy6ipz67q/victor_roris-worldcist2016.txt?dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.6.18, 2620:100:601c:18::a27d:612\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.6.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/dl/1mz0ociy6ipz67q/victor_roris-worldcist2016.txt [following]\n",
            "--2021-05-19 11:44:58--  https://www.dropbox.com/s/dl/1mz0ociy6ipz67q/victor_roris-worldcist2016.txt\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc6a62c3b2fccde64b1cf894c0ca.dl.dropboxusercontent.com/cd/0/get/BOxmxt3ajEznRcwiFf2y0-2OB8YaPh-6rgxfIdbC-A8kQYTPUt3P9Jh-dNT3gxGJoG_aaijJdhF2tU8z7wyLSh3PZ8TI_Vat7-N9S_Rv6Tc9C3GFJqHAyQDXFugph4wRLn7_J28LkDEVLFs19yaZjKCv/file?dl=1# [following]\n",
            "--2021-05-19 11:44:58--  https://uc6a62c3b2fccde64b1cf894c0ca.dl.dropboxusercontent.com/cd/0/get/BOxmxt3ajEznRcwiFf2y0-2OB8YaPh-6rgxfIdbC-A8kQYTPUt3P9Jh-dNT3gxGJoG_aaijJdhF2tU8z7wyLSh3PZ8TI_Vat7-N9S_Rv6Tc9C3GFJqHAyQDXFugph4wRLn7_J28LkDEVLFs19yaZjKCv/file?dl=1\n",
            "Resolving uc6a62c3b2fccde64b1cf894c0ca.dl.dropboxusercontent.com (uc6a62c3b2fccde64b1cf894c0ca.dl.dropboxusercontent.com)... 162.125.6.15, 2620:100:601c:15::a27d:60f\n",
            "Connecting to uc6a62c3b2fccde64b1cf894c0ca.dl.dropboxusercontent.com (uc6a62c3b2fccde64b1cf894c0ca.dl.dropboxusercontent.com)|162.125.6.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 25142 (25K) [application/binary]\n",
            "Saving to: ‘article.txt’\n",
            "\n",
            "article.txt         100%[===================>]  24.55K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2021-05-19 11:44:58 (17.7 MB/s) - ‘article.txt’ saved [25142/25142]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Hf8Qp46Ujs_"
      },
      "source": [
        "Read the content"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObFdiNydJMJ1"
      },
      "source": [
        "# Open a file: file\n",
        "content = \"\"\n",
        "with open('article.txt',mode='r') as file:\n",
        "  content = file.read()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KPCRrisJPog",
        "outputId": "620ec4ad-f548-4fd9-b680-bf58fa4b9f09"
      },
      "source": [
        "print(f\"Number of words : {len(content.split())}\")\n",
        "print(\"First lines:\")\n",
        "for line in content.split(\"\\n\")[0:3]:\n",
        "  print(line)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of words : 3830\n",
            "First lines:\n",
            "﻿________________\n",
            "A telematic based approach towards the normalization of clinical praxis\n",
            "Víctor M. Alonso Rorís1, Juan M. Santos Gago1, Luis Álvarez Sabucedo1, \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_8O2a1V3bs8"
      },
      "source": [
        "## Apply KEX\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odUoc9McOI9w",
        "outputId": "d8f015be-6da7-48e0-ed79-586ddbff7e08"
      },
      "source": [
        "import kex\n",
        "n_keywords = 10"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-05-19 11:44:59 INFO     'pattern' package not found; tag filters are not available for English\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMcb8SACOJP3"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJk5an5ANuR7"
      },
      "source": [
        "def print_keywords(keywords):\n",
        "  print(f\"Number of keywords : {len(keywords)}\")\n",
        "\n",
        "  keyword = keywords[0]\n",
        "\n",
        "  print(f\"- Parameters by keyword : {keyword.keys()}\")\n",
        "\n",
        "  print(\"- First keyword\")\n",
        "  print(f\"\\t - Keyword : {keyword['stemmed']}\")\n",
        "  print(f\"\\t - Number of ocurrences : {keyword['count']}\")\n",
        "  print(f\"\\t - Score : {keyword['score']}\")\n",
        "\n",
        "  print()\n",
        "  print(\"List of keywords\")\n",
        "  for idx, keyword in enumerate(keywords):\n",
        "    print(f\"{idx} [{keyword['score']}] {keyword['raw'][0]}\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlCpgchdOLRo"
      },
      "source": [
        "Built-in algorithms in kex is below:\n",
        "\n",
        "- FirstN: heuristic baseline to pick up first n phrases as keywords\n",
        "- TF: scoring by term frequency\n",
        "- TFIDF: scoring by TFIDF\n",
        "- LexSpec: scoring by lexical specificity\n",
        "- TextRank: Mihalcea et al., 04\n",
        "- SingleRank: Wan et al., 08\n",
        "- TopicalPageRank: Liu et al.,10\n",
        "- SingleTPR: Sterckx et al.,15\n",
        "- TopicRank: Bougouin et al.,13\n",
        "- PositionRank: Florescu et al.,18\n",
        "- TFIDFRank: SingleRank + TFIDF based word distribution prior\n",
        "- LexRank: SingleRank + lexical specificity based word distribution prior"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBsEfPO2d_vS"
      },
      "source": [
        "### Direct methods\n",
        "\n",
        "\n",
        "Algorithms such as `FirstN`, `TextRank`, `SingleRank`, `TopicRank` and `PositionRank` can be directly applied.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHxL2vlkHyyj",
        "outputId": "2ad39477-f04b-4ab1-f0f0-2a66b051722e"
      },
      "source": [
        "model = kex.FirstN() \n",
        "keywords = model.get_keywords(content, n_keywords=n_keywords)\n",
        "print_keywords(keywords)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of keywords : 10\n",
            "- Parameters by keyword : dict_keys(['stemmed', 'pos', 'raw', 'offset', 'count', 'score'])\n",
            "- First keyword\n",
            "\t - Keyword : approach\n",
            "\t - Number of ocurrences : 1\n",
            "\t - Score : 0\n",
            "\n",
            "List of keywords\n",
            "0 [0] approach\n",
            "1 [1] normalization\n",
            "2 [2] Luis Álvarez Sabucedo1\n",
            "3 [3] Mateo Ramos Merino1\n",
            "4 [4] Engineering Department\n",
            "5 [5] University\n",
            "6 [6] Vigo\n",
            "7 [7] 36310 Vigo\n",
            "8 [8] Spain\n",
            "9 [9] valonso\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMjtmaJTec4H",
        "outputId": "7c8e4f86-16d3-4f63-a040-5e7c25874388"
      },
      "source": [
        "model = kex.TextRank() \n",
        "keywords = model.get_keywords(content, n_keywords=n_keywords)\n",
        "print_keywords(keywords)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of keywords : 10\n",
            "- Parameters by keyword : dict_keys(['stemmed', 'pos', 'raw', 'offset', 'count', 'score', 'n_source_tokens'])\n",
            "- First keyword\n",
            "\t - Keyword : system\n",
            "\t - Number of ocurrences : 24\n",
            "\t - Score : 0.010023246097269418\n",
            "\n",
            "List of keywords\n",
            "0 [0.010023246097269418] system\n",
            "1 [0.009063705904145378] technological system\n",
            "2 [0.009020646588795533] information systems\n",
            "3 [0.008359524585289803] control systems\n",
            "4 [0.008104165711021339] technologies\n",
            "5 [0.008061106395671494] information technology\n",
            "6 [0.00801804708032165] information\n",
            "7 [0.00768183756774727] system description\n",
            "8 [0.007525717686871705] information operations\n",
            "9 [0.00750820812727075] process\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8hTsupYHxdN",
        "outputId": "2190bcbe-4ca3-4369-e5f4-95952cbc61fa"
      },
      "source": [
        "model = kex.SingleRank()  \n",
        "keywords = model.get_keywords(content, n_keywords=n_keywords)\n",
        "print_keywords(keywords)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of keywords : 10\n",
            "- Parameters by keyword : dict_keys(['stemmed', 'pos', 'raw', 'offset', 'count', 'score', 'n_source_tokens'])\n",
            "- First keyword\n",
            "\t - Keyword : system\n",
            "\t - Number of ocurrences : 24\n",
            "\t - Score : 0.00951135136685336\n",
            "\n",
            "List of keywords\n",
            "0 [0.00951135136685336] system\n",
            "1 [0.009316402856993903] operations\n",
            "2 [0.009193496709651451] control systems\n",
            "3 [0.00909602245472172] Control operations\n",
            "4 [0.00887564205244954] control\n",
            "5 [0.008746549543591418] technological system\n",
            "6 [0.008601789883868498] client\n",
            "7 [0.008568551318443526] information systems\n",
            "8 [0.008471077063513797] information operations\n",
            "9 [0.007981747720329476] technologies\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQt0yTeaeg5x",
        "outputId": "0dc6ecd6-f7d4-4ebb-9232-17b24f73d745"
      },
      "source": [
        "model = kex.TopicRank() \n",
        "keywords = model.get_keywords(content, n_keywords=n_keywords)\n",
        "print_keywords(keywords)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of keywords : 10\n",
            "- Parameters by keyword : dict_keys(['stemmed', 'pos', 'raw', 'offset', 'count', 'score', 'n_source_tokens'])\n",
            "- First keyword\n",
            "\t - Keyword : system\n",
            "\t - Number of ocurrences : 24\n",
            "\t - Score : 0.015863301489363928\n",
            "\n",
            "List of keywords\n",
            "0 [0.015863301489363928] system\n",
            "1 [0.013317010133330273] operating temperature\n",
            "2 [0.013174100570169357] human users\n",
            "3 [0.011725946689673762] control\n",
            "4 [0.010831990047865789] e.g.\n",
            "5 [0.010617600258692066] information technology\n",
            "6 [0.01045177655297573] PN\n",
            "7 [0.009041290380981006] procedures\n",
            "8 [0.008741900804153602] labels\n",
            "9 [0.008722486691391423] traces\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Op0M6DvgfkaO",
        "outputId": "42b05749-2685-4baf-b3a4-28dcaa71930a"
      },
      "source": [
        "model = kex.PositionRank() \n",
        "keywords = model.get_keywords(content, n_keywords=n_keywords)\n",
        "print_keywords(keywords)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of keywords : 10\n",
            "- Parameters by keyword : dict_keys(['stemmed', 'pos', 'raw', 'offset', 'count', 'score', 'n_source_tokens'])\n",
            "- First keyword\n",
            "\t - Keyword : M.\n",
            "\t - Number of ocurrences : 3\n",
            "\t - Score : 0.042495719917476305\n",
            "\n",
            "List of keywords\n",
            "0 [0.042495719917476305] M.\n",
            "1 [0.027713713851861645] 1\n",
            "2 [0.025290769818424177] J. M.\n",
            "3 [0.025269399780546892] V. M.\n",
            "4 [0.023274685911553985] M. A.\n",
            "5 [0.018997748507953865] ad-hoc telematics solutions\n",
            "6 [0.018236219614993644] Clinical\n",
            "7 [0.018011054269844104] clinical praxis Víctor M. Alonso Rorís1\n",
            "8 [0.017641874985082736] Juan M. Santos Gago1\n",
            "9 [0.0166193427185386] Álvarez\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZL_ULOIJP0pV"
      },
      "source": [
        "### Compute a statistical prior\n",
        "\n",
        "Algorithms such as `TF`, `TFIDF`, `TFIDFRank`, `LexSpec`, `LexRank`, `TopicalPageRank`, and `SingleTPR` need to compute a prior distribution beforehand by"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rskuCJBmRBho",
        "outputId": "85c7d865-498c-44ec-b43f-36e51192d9fc"
      },
      "source": [
        "import nltk\n",
        "import nltk.data\n",
        "nltk.download('punkt')\n",
        "\n",
        "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "sentences = tokenizer.tokenize(content)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jeGgWg8jOjQp",
        "outputId": "828ba6c3-0aea-46f9-fa33-631582d4c2ce"
      },
      "source": [
        "model = kex.TF() \n",
        "model.train(sentences, export_directory='./tmp')\n",
        "keywords = model.get_keywords(content, n_keywords=n_keywords)\n",
        "print_keywords(keywords)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of keywords : 10\n",
            "- Parameters by keyword : dict_keys(['stemmed', 'pos', 'raw', 'offset', 'count', 'score', 'n_source_tokens'])\n",
            "- First keyword\n",
            "\t - Keyword : system\n",
            "\t - Number of ocurrences : 24\n",
            "\t - Score : 37.0\n",
            "\n",
            "List of keywords\n",
            "0 [37.0] system\n",
            "1 [34.5] control systems\n",
            "2 [34.0] operations\n",
            "3 [33.0] Control operations\n",
            "4 [32.0] control\n",
            "5 [30.0] information systems\n",
            "6 [28.5] information operations\n",
            "7 [26.0] Users\n",
            "8 [25.0] system description\n",
            "9 [24.5] technological system\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJMGB00GHy1W",
        "outputId": "f46b8196-5366-401e-cefc-e266b13b1487"
      },
      "source": [
        "model = kex.TFIDF() \n",
        "model.train(sentences, export_directory='./tmp')\n",
        "keywords = model.get_keywords(content, n_keywords=n_keywords)\n",
        "print_keywords(keywords)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-05-19 11:45:02 INFO     adding document #0 to Dictionary(0 unique tokens: [])\n",
            "2021-05-19 11:45:02 INFO     built Dictionary(897 unique tokens: ['&', ',', '.', '03550', '1']...) from 246 documents (total 3153 corpus positions)\n",
            "2021-05-19 11:45:02 INFO     collecting document frequencies\n",
            "2021-05-19 11:45:02 INFO     PROGRESS: processing document #0\n",
            "2021-05-19 11:45:02 INFO     calculating IDF weights for 246 documents and 896 features (2859 matrix non-zeros)\n",
            "2021-05-19 11:45:02 INFO     saving TfidfModel object under ./tmp/tfidf_model, separately None\n",
            "2021-05-19 11:45:02 INFO     saved ./tmp/tfidf_model\n",
            "2021-05-19 11:45:02 INFO     saving dictionary mapping to ./tmp/tfidf_dict\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number of keywords : 10\n",
            "- Parameters by keyword : dict_keys(['stemmed', 'pos', 'raw', 'offset', 'count', 'score', 'n_source_tokens'])\n",
            "- First keyword\n",
            "\t - Keyword : oper\n",
            "\t - Number of ocurrences : 21\n",
            "\t - Score : 0.14893716050988023\n",
            "\n",
            "List of keywords\n",
            "0 [0.14893716050988023] operations\n",
            "1 [0.1456873886164094] system\n",
            "2 [0.1423704106854542] Control operations\n",
            "3 [0.14074552473871876] control systems\n",
            "4 [0.13580366086102816] control\n",
            "5 [0.13135338998251403] information operations\n",
            "6 [0.12972850403577862] information systems\n",
            "7 [0.12179945368644407] Users\n",
            "8 [0.11376961945514784] information\n",
            "9 [0.11200222045350436] system description\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkzRrIbnHy4W",
        "outputId": "6de464ff-df88-46e0-86c3-c14e7ffa9d6a"
      },
      "source": [
        "model = kex.TFIDFRank() \n",
        "model.train(sentences, export_directory='./tmp')\n",
        "keywords = model.get_keywords(content, n_keywords=n_keywords)\n",
        "print_keywords(keywords)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-05-19 11:45:04 INFO     adding document #0 to Dictionary(0 unique tokens: [])\n",
            "2021-05-19 11:45:04 INFO     built Dictionary(897 unique tokens: ['&', ',', '.', '03550', '1']...) from 246 documents (total 3153 corpus positions)\n",
            "2021-05-19 11:45:04 INFO     collecting document frequencies\n",
            "2021-05-19 11:45:04 INFO     PROGRESS: processing document #0\n",
            "2021-05-19 11:45:04 INFO     calculating IDF weights for 246 documents and 896 features (2859 matrix non-zeros)\n",
            "2021-05-19 11:45:04 INFO     saving TfidfModel object under ./tmp/tfidf_model, separately None\n",
            "2021-05-19 11:45:04 INFO     saved ./tmp/tfidf_model\n",
            "2021-05-19 11:45:04 INFO     saving dictionary mapping to ./tmp/tfidf_dict\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number of keywords : 10\n",
            "- Parameters by keyword : dict_keys(['stemmed', 'pos', 'raw', 'offset', 'count', 'score', 'n_source_tokens'])\n",
            "- First keyword\n",
            "\t - Keyword : oper\n",
            "\t - Number of ocurrences : 21\n",
            "\t - Score : 0.014513331529947682\n",
            "\n",
            "List of keywords\n",
            "0 [0.014513331529947682] operations\n",
            "1 [0.01427188260367997] Control operations\n",
            "2 [0.014030433677412257] control\n",
            "3 [0.013836296749628656] control systems\n",
            "4 [0.013642159821845056] system\n",
            "5 [0.012671743174924793] information operations\n",
            "6 [0.012236157320873482] information systems\n",
            "7 [0.012026474425472382] client\n",
            "8 [0.011676799411742099] technological system\n",
            "9 [0.010830154819901905] information\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Mgt01TYRqFQ",
        "outputId": "51ee62cf-1e58-474c-de10-7a8e6e5d4f0f"
      },
      "source": [
        "model = kex.LexSpec() \n",
        "model.train(sentences, export_directory='./tmp')\n",
        "keywords = model.get_keywords(content[:11500], n_keywords=n_keywords) # It fails with all the text\n",
        "print_keywords(keywords)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of keywords : 10\n",
            "- Parameters by keyword : dict_keys(['stemmed', 'pos', 'raw', 'offset', 'count', 'score', 'n_source_tokens'])\n",
            "- First keyword\n",
            "\t - Keyword : server\n",
            "\t - Number of ocurrences : 4\n",
            "\t - Score : 1.291104119772643\n",
            "\n",
            "List of keywords\n",
            "0 [1.291104119772643] server\n",
            "1 [1.1954150930170626] recording\n",
            "2 [1.1064416734547526] mechanisms\n",
            "3 [0.924830053488585] entities\n",
            "4 [0.6455520598863215] server functions\n",
            "5 [0.6455520598863215] server agent\n",
            "6 [0.5996007124997992] recorded information\n",
            "7 [0.5977075465085313] traceable records\n",
            "8 [0.5977075465085313] auditable record\n",
            "9 [0.5532208367273763] invocation mechanisms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Da2GJmKxRvKa",
        "outputId": "cac6748c-0d7d-41ff-a55d-d60b0590f20b"
      },
      "source": [
        "model = kex.LexRank() \n",
        "model.train(sentences, export_directory='./tmp')\n",
        "keywords = model.get_keywords(content[:11500], n_keywords=n_keywords) # It fails with all the text\n",
        "print_keywords(keywords)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of keywords : 10\n",
            "- Parameters by keyword : dict_keys(['stemmed', 'pos', 'raw', 'offset', 'count', 'score', 'n_source_tokens'])\n",
            "- First keyword\n",
            "\t - Keyword : system\n",
            "\t - Number of ocurrences : 19\n",
            "\t - Score : 0.03041609828790691\n",
            "\n",
            "List of keywords\n",
            "0 [0.03041609828790691] system\n",
            "1 [0.02832000843073278] agents\n",
            "2 [0.02683221457198216] entities\n",
            "3 [0.0257337302241822] client agents\n",
            "4 [0.02333211196445681] server agent\n",
            "5 [0.02314745201763162] client\n",
            "6 [0.023083767551911247] system description\n",
            "7 [0.021330364707705406] HTTP\n",
            "8 [0.020784550096836097] monitored variables\n",
            "9 [0.020771119619909992] information systems\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9FF7O5PqTuzc",
        "outputId": "81fd90f4-3912-4ca1-ed19-e561880f2666"
      },
      "source": [
        "model = kex.TopicalPageRank() \n",
        "model.train(sentences, export_directory='./tmp')\n",
        "\n",
        "## There is some error in the implementation. The `build_graph` method returns 4\n",
        "## params but the code expects only 3. I modified manually the library to accept\n",
        "## an extra unuseless param:\n",
        "# graph, phrase_instance, original_sentence_token_size, _ = self.build_graph(document)\n",
        "\n",
        "keywords = model.get_keywords(content, n_keywords=n_keywords) \n",
        "print_keywords(keywords)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-05-19 11:45:07 INFO     adding document #0 to Dictionary(0 unique tokens: [])\n",
            "2021-05-19 11:45:07 INFO     built Dictionary(897 unique tokens: ['&', ',', '.', '03550', '1']...) from 246 documents (total 3153 corpus positions)\n",
            "2021-05-19 11:45:07 INFO     using symmetric alpha at 0.06666666666666667\n",
            "2021-05-19 11:45:07 INFO     using symmetric eta at 0.06666666666666667\n",
            "2021-05-19 11:45:07 INFO     using serial LDA version on this node\n",
            "2021-05-19 11:45:07 INFO     running online (single-pass) LDA training, 15 topics, 1 passes over the supplied corpus of 246 documents, updating model once every 246 documents, evaluating perplexity every 246 documents, iterating 50x with a convergence threshold of 0.001000\n",
            "2021-05-19 11:45:07 WARNING  too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "2021-05-19 11:45:07 INFO     -13.721 per-word bound, 13506.0 perplexity estimate based on a held-out corpus of 246 documents with 3153 words\n",
            "2021-05-19 11:45:07 INFO     PROGRESS: pass 0, at document #246/246\n",
            "2021-05-19 11:45:07 INFO     topic #2 (0.067): 0.106*\",\" + 0.052*\".\" + 0.025*\"system\" + 0.023*\"thi\" + 0.020*\"(\" + 0.020*\"inform\" + 0.017*\"control\" + 0.017*\"e.g.\" + 0.015*\"entiti\" + 0.015*\")\"\n",
            "2021-05-19 11:45:07 INFO     topic #11 (0.067): 0.088*\",\" + 0.077*\".\" + 0.025*\")\" + 0.016*\"(\" + 0.016*\"thi\" + 0.014*\"oper\" + 0.012*\"PN\" + 0.012*\"user\" + 0.011*\"propos\" + 0.010*\";\"\n",
            "2021-05-19 11:45:07 INFO     topic #4 (0.067): 0.037*\".\" + 0.019*\",\" + 0.019*\"follow\" + 0.019*\"gener\" + 0.019*\"server\" + 0.010*\"fig\" + 0.010*\"control\" + 0.010*\"shown\" + 0.010*\"characterist\" + 0.010*\"procedur\"\n",
            "2021-05-19 11:45:07 INFO     topic #8 (0.067): 0.068*\".\" + 0.048*\",\" + 0.021*\"user\" + 0.021*\"oper\" + 0.014*\"control\" + 0.014*\"thi\" + 0.014*\"procedur\" + 0.014*\"system\" + 0.014*\"descript\" + 0.014*\"CP\"\n",
            "2021-05-19 11:45:07 INFO     topic #14 (0.067): 0.066*\".\" + 0.024*\",\" + 0.021*\"oper\" + 0.019*\"trace\" + 0.014*\"qualiti\" + 0.014*\"real\" + 0.014*\"user\" + 0.014*\"time\" + 0.014*\"check\" + 0.014*\")\"\n",
            "2021-05-19 11:45:07 INFO     topic diff=11.429071, rho=1.000000\n",
            "2021-05-19 11:45:07 INFO     saving LdaState object under ./tmp/lda_model.state, separately None\n",
            "2021-05-19 11:45:07 INFO     saved ./tmp/lda_model.state\n",
            "2021-05-19 11:45:07 INFO     saving LdaModel object under ./tmp/lda_model, separately ['expElogbeta', 'sstats']\n",
            "2021-05-19 11:45:07 INFO     storing np array 'expElogbeta' to ./tmp/lda_model.expElogbeta.npy\n",
            "2021-05-19 11:45:07 INFO     not storing attribute dispatcher\n",
            "2021-05-19 11:45:07 INFO     not storing attribute id2word\n",
            "2021-05-19 11:45:07 INFO     not storing attribute state\n",
            "2021-05-19 11:45:07 INFO     saved ./tmp/lda_model\n",
            "2021-05-19 11:45:07 INFO     saving dictionary mapping to ./tmp/lda_dict\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "4\n",
            "Number of keywords : 10\n",
            "- Parameters by keyword : dict_keys(['stemmed', 'pos', 'raw', 'offset', 'count', 'score', 'n_source_tokens'])\n",
            "- First keyword\n",
            "\t - Keyword : control oper defin variabl\n",
            "\t - Number of ocurrences : 1\n",
            "\t - Score : 0.03879468693711062\n",
            "\n",
            "List of keywords\n",
            "0 [0.03879468693711062] control operation defines variables\n",
            "1 [0.03451257219321824] control process procedures\n",
            "2 [0.0319067860816122] control systems\n",
            "3 [0.031685053758973764] Control operations\n",
            "4 [0.030483723020960727] Standard Operating Procedures\n",
            "5 [0.029533370625683952] client application agent\n",
            "6 [0.028246981884026336] generic client agent\n",
            "7 [0.027196593246606168] information systems\n",
            "8 [0.027095874492829033] parenteral nutrition systems\n",
            "9 [0.026974860923967732] information operations\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKriYR3mUKSP",
        "outputId": "e00683a0-d749-44be-b82e-16240ac92e4a"
      },
      "source": [
        "model = kex.SingleTPR() \n",
        "model.train(sentences, export_directory='./tmp')\n",
        "keywords = model.get_keywords(content, n_keywords=n_keywords) \n",
        "print_keywords(keywords)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-05-19 11:51:31 INFO     adding document #0 to Dictionary(0 unique tokens: [])\n",
            "2021-05-19 11:51:31 INFO     built Dictionary(897 unique tokens: ['&', ',', '.', '03550', '1']...) from 246 documents (total 3153 corpus positions)\n",
            "2021-05-19 11:51:31 INFO     using symmetric alpha at 0.06666666666666667\n",
            "2021-05-19 11:51:31 INFO     using symmetric eta at 0.06666666666666667\n",
            "2021-05-19 11:51:31 INFO     using serial LDA version on this node\n",
            "2021-05-19 11:51:31 INFO     running online (single-pass) LDA training, 15 topics, 1 passes over the supplied corpus of 246 documents, updating model once every 246 documents, evaluating perplexity every 246 documents, iterating 50x with a convergence threshold of 0.001000\n",
            "2021-05-19 11:51:31 WARNING  too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "2021-05-19 11:51:31 INFO     -13.737 per-word bound, 13652.2 perplexity estimate based on a held-out corpus of 246 documents with 3153 words\n",
            "2021-05-19 11:51:31 INFO     PROGRESS: pass 0, at document #246/246\n",
            "2021-05-19 11:51:31 INFO     topic #10 (0.067): 0.070*\",\" + 0.063*\".\" + 0.025*\"oper\" + 0.019*\"control\" + 0.013*\"system\" + 0.013*\"critic\" + 0.013*\"record\" + 0.013*\"monitor\" + 0.013*\"involv\" + 0.013*\"user\"\n",
            "2021-05-19 11:51:31 INFO     topic #12 (0.067): 0.039*\",\" + 0.039*\".\" + 0.032*\";\" + 0.017*\"(\" + 0.016*\")\" + 0.012*\"variabl\" + 0.012*\"label\" + 0.011*\":\" + 0.010*\"user\" + 0.010*\"project\"\n",
            "2021-05-19 11:51:31 INFO     topic #5 (0.067): 0.047*\".\" + 0.021*\",\" + 0.021*\":\" + 0.021*\"architectur\" + 0.011*\"applic\" + 0.011*\"system\" + 0.011*\"document\" + 0.011*\"frame\" + 0.011*\"test\" + 0.011*\"scenario\"\n",
            "2021-05-19 11:51:31 INFO     topic #14 (0.067): 0.064*\".\" + 0.048*\")\" + 0.039*\",\" + 0.026*\"(\" + 0.023*\":\" + 0.020*\"system\" + 0.016*\"gener\" + 0.016*\"applic\" + 0.015*\"PN\" + 0.013*\"control\"\n",
            "2021-05-19 11:51:31 INFO     topic #7 (0.067): 0.041*\")\" + 0.037*\".\" + 0.034*\"(\" + 0.024*\";\" + 0.016*\"medic\" + 0.015*\"journal\" + 0.013*\"number\" + 0.012*\",\" + 0.011*\"behavior\" + 0.011*\"analysi\"\n",
            "2021-05-19 11:51:31 INFO     topic diff=11.483150, rho=1.000000\n",
            "2021-05-19 11:51:31 INFO     saving LdaState object under ./tmp/lda_model.state, separately None\n",
            "2021-05-19 11:51:31 INFO     saved ./tmp/lda_model.state\n",
            "2021-05-19 11:51:31 INFO     saving LdaModel object under ./tmp/lda_model, separately ['expElogbeta', 'sstats']\n",
            "2021-05-19 11:51:31 INFO     storing np array 'expElogbeta' to ./tmp/lda_model.expElogbeta.npy\n",
            "2021-05-19 11:51:31 INFO     not storing attribute dispatcher\n",
            "2021-05-19 11:51:31 INFO     not storing attribute id2word\n",
            "2021-05-19 11:51:31 INFO     not storing attribute state\n",
            "2021-05-19 11:51:31 INFO     saved ./tmp/lda_model\n",
            "2021-05-19 11:51:31 INFO     saving dictionary mapping to ./tmp/lda_dict\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number of keywords : 10\n",
            "- Parameters by keyword : dict_keys(['stemmed', 'pos', 'raw', 'offset', 'count', 'score', 'n_source_tokens'])\n",
            "- First keyword\n",
            "\t - Keyword : oper\n",
            "\t - Number of ocurrences : 21\n",
            "\t - Score : 0.016645899118662582\n",
            "\n",
            "List of keywords\n",
            "0 [0.016645899118662582] operations\n",
            "1 [0.01610051087992711] Control operations\n",
            "2 [0.015555122641191632] control\n",
            "3 [0.014931230333202268] control systems\n",
            "4 [0.014307338025212906] system\n",
            "5 [0.014038391837220335] information operations\n",
            "6 [0.012911072153984123] client\n",
            "7 [0.012869111290495498] information systems\n",
            "8 [0.011962486579538929] technological system\n",
            "9 [0.011439380176468741] control process procedures\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvRfkolYd3Ca"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}