{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KeywordExtraction-RAKE&YAKE.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOEGVnOgdb07wzv809aYFzY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/victor-roris/NLPlearning/blob/master/summarization-keywords/KeywordExtraction_RAKE%26YAKE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u58G50zcRoIs"
      },
      "source": [
        "# Basic Keyword Extracton Algorithms: RAKE and YAKE\n",
        "\n",
        "More information: https://amitness.com/keyphrase-extraction/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zt0Y9U6BSOH8"
      },
      "source": [
        "## Document of study\n",
        "\n",
        "We are going to apply keyword Extraction algorithms in a specific text. The idea is use always the same content to study the different results. At same time, it is important know the document to evaluate if the results are valid or not. \n",
        "\n",
        "To reach this goal, we are going to use an scientific article text. Furthermore, we remove the abstract and the keywords of the content.\n",
        "\n",
        "The authors labelled the document with the keywords:\n",
        "\n",
        " ```traceability, eHealth, software platform, mobile environments```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuF9hHkNOHdJ"
      },
      "source": [
        "Download the text file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOzgy9l-INWF",
        "outputId": "4be1d62d-eb0e-4b32-cbc9-0985a86ee39a"
      },
      "source": [
        "!wget -O article.txt https://www.dropbox.com/s/1mz0ociy6ipz67q/victor_roris-worldcist2016.txt?dl=1 "
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-18 16:07:17--  https://www.dropbox.com/s/1mz0ociy6ipz67q/victor_roris-worldcist2016.txt?dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.18, 2620:100:6018:18::a27d:312\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/dl/1mz0ociy6ipz67q/victor_roris-worldcist2016.txt [following]\n",
            "--2021-05-18 16:07:17--  https://www.dropbox.com/s/dl/1mz0ociy6ipz67q/victor_roris-worldcist2016.txt\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ucc802c9f80c4bdc3f931f51edaa.dl.dropboxusercontent.com/cd/0/get/BOu6leoYAPK7v4vwCo_yibDHXGrq3Gf3q6FRvT6dLKwmKwSUS8ROe-LOZwHgiozhpm3By9DDkdPIjWaZ3k0AlqJn5snExymsg_-_ox6DKSI1Ho2Vze7wl1bwiCQ-qk2yTyGgzgTamtW1ilmSJUjkSG2M/file?dl=1# [following]\n",
            "--2021-05-18 16:07:17--  https://ucc802c9f80c4bdc3f931f51edaa.dl.dropboxusercontent.com/cd/0/get/BOu6leoYAPK7v4vwCo_yibDHXGrq3Gf3q6FRvT6dLKwmKwSUS8ROe-LOZwHgiozhpm3By9DDkdPIjWaZ3k0AlqJn5snExymsg_-_ox6DKSI1Ho2Vze7wl1bwiCQ-qk2yTyGgzgTamtW1ilmSJUjkSG2M/file?dl=1\n",
            "Resolving ucc802c9f80c4bdc3f931f51edaa.dl.dropboxusercontent.com (ucc802c9f80c4bdc3f931f51edaa.dl.dropboxusercontent.com)... 162.125.3.15, 2620:100:6018:15::a27d:30f\n",
            "Connecting to ucc802c9f80c4bdc3f931f51edaa.dl.dropboxusercontent.com (ucc802c9f80c4bdc3f931f51edaa.dl.dropboxusercontent.com)|162.125.3.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 25843 (25K) [application/binary]\n",
            "Saving to: ‘article.txt’\n",
            "\n",
            "article.txt         100%[===================>]  25.24K  --.-KB/s    in 0.009s  \n",
            "\n",
            "2021-05-18 16:07:18 (2.60 MB/s) - ‘article.txt’ saved [25843/25843]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Hf8Qp46Ujs_"
      },
      "source": [
        "Read the content"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObFdiNydJMJ1"
      },
      "source": [
        "# Open a file: file\n",
        "content = \"\"\n",
        "with open('article.txt',mode='r') as file:\n",
        "  content = file.read()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KPCRrisJPog",
        "outputId": "16052b40-6142-4cfd-a75d-61971d28d5d9"
      },
      "source": [
        "print(f\"Number of words : {len(content.split())}\")\n",
        "print(\"First lines:\")\n",
        "for line in content.split(\"\\n\")[0:3]:\n",
        "  print(line)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of words : 3930\n",
            "First lines:\n",
            "﻿________________\n",
            "A telematic based approach towards the normalization of clinical praxis\n",
            "Víctor M. Alonso Rorís1, Juan M. Santos Gago1, Luis Álvarez Sabucedo1, \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siSOLRk0Euwc"
      },
      "source": [
        "## RAKE: Rapid Automatic Keyword Extraction Algorithm\n",
        "\n",
        "Rake is based on the observations that keywords frequently contain multiple words with standard punctuation or stop words.\n",
        "\n",
        "Rake uses stop words and phrase delimiters to partition the document into candidate keywords."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66ViFwSlFyTa"
      },
      "source": [
        "### Install RAKE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBsMIeIXjdXW",
        "outputId": "4d5e8253-7746-4b0f-e0db-c92d2d11ef77"
      },
      "source": [
        "!pip install rake-nltk"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting rake-nltk\n",
            "  Downloading https://files.pythonhosted.org/packages/8e/c4/b4ff57e541ac5624ad4b20b89c2bafd4e98f29fd83139f3a81858bdb3815/rake_nltk-1.0.4.tar.gz\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from rake-nltk) (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->rake-nltk) (1.15.0)\n",
            "Building wheels for collected packages: rake-nltk\n",
            "  Building wheel for rake-nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rake-nltk: filename=rake_nltk-1.0.4-py2.py3-none-any.whl size=7819 sha256=03786d91daf43610d110fcddf27cb3465cf7738502709bbbe3ab7383bbbdf553\n",
            "  Stored in directory: /root/.cache/pip/wheels/ef/92/fc/271b3709e71a96ffe934b27818946b795ac6b9b8ff8682483f\n",
            "Successfully built rake-nltk\n",
            "Installing collected packages: rake-nltk\n",
            "Successfully installed rake-nltk-1.0.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7NMwlJMlGz3E",
        "outputId": "24646f1e-85fa-4471-a203-f51452a2bdca"
      },
      "source": [
        "!python -c \"import nltk; nltk.download('stopwords')\""
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlRK44HOU9OD"
      },
      "source": [
        "### Apply Rake"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvDUYdBKGZbS",
        "outputId": "e83fee67-52a6-4f37-90af-e0d05a6b13cb"
      },
      "source": [
        "from rake_nltk import Rake\n",
        "\n",
        "r = Rake() # Uses stopwords for english from NLTK, and all puntuation characters.\n",
        "\n",
        "r.extract_keywords_from_text(content)\n",
        "\n",
        "r.get_ranked_phrases_with_scores()[0:10]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(38.95, 'javier sanz valero2 1 telematic engineering department'),\n",
              " (30.40705128205128, '2 suppl ), 20s – 22s'),\n",
              " (30.40705128205128, '2 suppl ), 10s – 13s'),\n",
              " (29.237373737373737, 'data using external information shared publicly'),\n",
              " (25.06767676767677, 'g ., using inference engines ).'),\n",
              " (25.0, 'sus cuidadores con la unidad'),\n",
              " (24.166666666666668, 'instituto de salud carlos iii'),\n",
              " (22.40705128205128, '2 ), 95 – 104'),\n",
              " (22.375, 'server functions via secure connections'),\n",
              " (21.68181818181818, 'drug component using data extracted')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apsBm5s1VBEz"
      },
      "source": [
        "## YAKE: Yet Another Keyword Extractor\n",
        "\n",
        "YAKE uses statistical features to identify and rank the most important keywords. It doesn’t need any linguistic information like NER or POS tagging and thus can be used with any language. It only requires a stop word list for the language."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_8r-WLLVXZP"
      },
      "source": [
        "### Install YAKE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BalNz_eRIAU"
      },
      "source": [
        "!pip install git+https://github.com/boudinfl/pke.git\n",
        "!python -m nltk.downloader stopwords\n",
        "!python -m spacy download en"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdxN4uQnVayl"
      },
      "source": [
        "### Apply YAKE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZOC3rZXRICp",
        "outputId": "fb340eb4-32fc-4e6e-f80c-5f0b63fff1fd"
      },
      "source": [
        "from pke.unsupervised import YAKE\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "document = content\n",
        "\n",
        "# 1. Create YAKE keyword extractor\n",
        "extractor = YAKE()\n",
        "\n",
        "# 2. Load document\n",
        "extractor.load_document(input=document,\n",
        "                        language='en',\n",
        "                        normalization=None)\n",
        "\n",
        "\n",
        "# 3. Generate candidate 1-gram, 2-gram  and 3-gram keywords\n",
        "stoplist = stopwords.words('english')\n",
        "extractor.candidate_selection(n=3, stoplist=stoplist)\n",
        "\n",
        "# 4. Calculate scores for the candidate keywords\n",
        "extractor.candidate_weighting(window=3,\n",
        "                              stoplist=stoplist,\n",
        "                              use_stems=False)\n",
        "\n",
        "# 5. Select 10 highest ranked keywords\n",
        "# Remove redundant keywords with similarity above 80%\n",
        "key_phrases = extractor.get_n_best(n=10, threshold=0.8)\n",
        "key_phrases"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('assembling of evidences', 0.0),\n",
              " ('open and interoperable', 0.0),\n",
              " ('mateo ramos merino1', 0.0003883911645799604),\n",
              " ('university miguel hernandez', 0.002455921040534705),\n",
              " ('luis álvarez sabucedo1', 0.002581925467948929),\n",
              " ('telematic engineering department', 0.0028370062555477986),\n",
              " ('javier sanz valero2', 0.0035285959073381814),\n",
              " ('mateo ramos', 0.005303752134611906),\n",
              " ('ramos merino1', 0.005303752134611906),\n",
              " ('engineering department', 0.005303752134611906)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awarzHKaRIFb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}