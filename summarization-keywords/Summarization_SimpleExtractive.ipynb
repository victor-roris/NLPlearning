{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Summarization-SimpleExtractive.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOaUuMyZ//1G0F2fqqNGR0S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/victor-roris/NLPlearning/blob/master/summarization-keywords/Summarization_SimpleExtractive.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srbS2qjYyjyD"
      },
      "source": [
        "# Summarization using a Simple Extractive method\n",
        "\n",
        "Input article → split into sentences → remove stop words → build a similarity matrix → generate rank based on matrix → pick top N sentences for summary.\n",
        "\n",
        "From: https://towardsdatascience.com/understand-text-summarization-and-create-your-own-summarizer-in-python-b26a9f09fc70"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MHl1YYgCK0-"
      },
      "source": [
        "## Install"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUwdZDqJyj_T"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zt0Y9U6BSOH8"
      },
      "source": [
        "## Document of study\n",
        "\n",
        "We are going to apply keyword Extraction algorithms in a specific text. The idea is use always the same content to study the different results. At same time, it is important know the document to evaluate if the results are valid or not. \n",
        "\n",
        "To reach this goal, we are going to use an scientific article text. Furthermore, we removed the abstract and the keywords of the content.\n",
        "\n",
        "The authors labelled the document with the abstract and keywords:\n",
        "\n",
        "* **Abstract**: The provision of comprehensive support for traceability and control is a raising demand in some environments such as the eHealth domain where processes can be of critical importance. This paper provides a detailed and thoughtful description of a holistic platform for the characterization and control of processes in the frame of the HACCP context. Traceability features are fully integrated in the model along with support for services concerned with information for the platform users. These features are provided using already tested technologies (RESTful models, QR Codes) and low cost devices (regular smartphones).\n",
        "\n",
        "* **Keywords**: traceability, eHealth, software platform, mobile environments\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuF9hHkNOHdJ"
      },
      "source": [
        "Download the text file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOzgy9l-INWF"
      },
      "source": [
        "!wget -O article.txt https://www.dropbox.com/s/1mz0ociy6ipz67q/victor_roris-worldcist2016.txt?dl=1 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Hf8Qp46Ujs_"
      },
      "source": [
        "Read the content"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObFdiNydJMJ1"
      },
      "source": [
        "# Open a file: file\n",
        "content = \"\"\n",
        "with open('article.txt',mode='r') as file:\n",
        "  content = file.read()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KPCRrisJPog",
        "outputId": "24a192aa-c789-4efb-8a75-c578276718d6"
      },
      "source": [
        "print(f\"Number of words : {len(content.split())}\")\n",
        "print(\"First lines:\")\n",
        "for line in content.split(\"\\n\")[0:3]:\n",
        "  print(line)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of words : 3830\n",
            "First lines:\n",
            "﻿________________\n",
            "A telematic based approach towards the normalization of clinical praxis\n",
            "Víctor M. Alonso Rorís1, Juan M. Santos Gago1, Luis Álvarez Sabucedo1, \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_8O2a1V3bs8"
      },
      "source": [
        "## Apply "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMDNnudlKXbx"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.cluster.util import cosine_distance\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import numpy as np\n",
        "import networkx as nx"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvjIF3teQZT0",
        "outputId": "aaf50b8c-4c37-44e6-81fd-a0abbd0fb7c1"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEMW0u--Kfpn"
      },
      "source": [
        "Generate clean sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjT1sh9pKbnb"
      },
      "source": [
        "def get_sentences(text):\n",
        "    article = sent_tokenize(text)\n",
        "    sentences = []\n",
        "    for sentence in article:\n",
        "      sentences.append(sentence.replace(\"[^a-zA-Z]\", \" \").split(\" \"))\n",
        "    sentences.pop() \n",
        "    \n",
        "    return sentences"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJDiE7ghU4Zm"
      },
      "source": [
        "Sentence similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpAkqJ5XU4gr"
      },
      "source": [
        "def sentence_similarity(sent1, sent2, stopwords=None):\n",
        "    if stopwords is None:\n",
        "        stopwords = []\n",
        " \n",
        "    sent1 = [w.lower() for w in sent1]\n",
        "    sent2 = [w.lower() for w in sent2]\n",
        " \n",
        "    all_words = list(set(sent1 + sent2))\n",
        " \n",
        "    vector1 = [0] * len(all_words)\n",
        "    vector2 = [0] * len(all_words)\n",
        " \n",
        "    # build the vector for the first sentence\n",
        "    for w in sent1:\n",
        "        if w in stopwords:\n",
        "            continue\n",
        "        vector1[all_words.index(w)] += 1\n",
        " \n",
        "    # build the vector for the second sentence\n",
        "    for w in sent2:\n",
        "        if w in stopwords:\n",
        "            continue\n",
        "        vector2[all_words.index(w)] += 1\n",
        " \n",
        "    return 1 - cosine_distance(vector1, vector2)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huNR06mgKqIq"
      },
      "source": [
        "Similarity matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOSPMRwLKjdl"
      },
      "source": [
        "def build_similarity_matrix(sentences, stop_words):\n",
        "    # Create an empty similarity matrix\n",
        "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
        " \n",
        "    for idx1 in range(len(sentences)):\n",
        "        for idx2 in range(len(sentences)):\n",
        "            if idx1 == idx2: #ignore if both are same sentences\n",
        "                continue \n",
        "            similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2], stop_words)\n",
        "    return similarity_matrix"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgsjQi8lKyJu"
      },
      "source": [
        "Generate Summary Method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNgbX0LOKsh3"
      },
      "source": [
        "def generate_summary(content, top_n=5):\n",
        "\n",
        "    stop_words = stopwords.words('english')\n",
        "    summarize_text = []\n",
        "    \n",
        "    # Step 1 - Read text and tokenize\n",
        "    sentences =  get_sentences(content)\n",
        "    \n",
        "    # Step 2 - Generate Similary Martix across sentences\n",
        "    sentence_similarity_martix = build_similarity_matrix(sentences, stop_words)\n",
        "    \n",
        "    # Step 3 - Rank sentences in similarity martix\n",
        "    sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_martix)\n",
        "    scores = nx.pagerank(sentence_similarity_graph)\n",
        "    \n",
        "    # Step 4 - Sort the rank and pick top sentences\n",
        "    ranked_sentence = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)    \n",
        "    # print(\"Indexes of top ranked_sentence order are \", ranked_sentence)\n",
        "    for i in range(top_n):\n",
        "      summarize_text.append(\" \".join(ranked_sentence[i][1]))\n",
        "    \n",
        "    # Step 5 - Offcourse, output the summarize texr\n",
        "    print(\"Summarize Text: \\n\", \". \".join(summarize_text))"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvblaORlQKuN",
        "outputId": "f38dd59f-71a6-454f-dc36-80a025776016"
      },
      "source": [
        "# let's begin\n",
        "generate_summary(content, 2)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Summarize Text: \n",
            " To do this, the system must provide with tools to control the entire life cycle of procedures and entities (e.g., medication).. In this case, the use of semantic technologies facilitates the implementation of  enriching  techniques for population of data using external information shared publicly on the Web (e.g., adding information about a drug component using data extracted from Wikipedia/DBPedia).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35DHPFNLWbaw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}