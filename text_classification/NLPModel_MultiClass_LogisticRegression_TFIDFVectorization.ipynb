{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLPModel_MultiClass_LogisticRegression-TFIDFVectorization.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/victor-roris/mediumseries/blob/master/NLP/NLPModel_MultiLabel_LogisticRegression_TFIDFVectorization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckNDa9TRtsuy",
        "colab_type": "text"
      },
      "source": [
        "# NLP Model with Logistic Regression \n",
        "\n",
        "In this notebook we are going to train a Logistic Regression Model to predict categories of text. To vectorize the text we are going to use the TFIDF approach. \n",
        "\n",
        "Sklearn Logistic Regression: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html \n",
        "\n",
        "Sklearn TFIDF: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
        "\n",
        "Notebook adapted from: https://github.com/makcedward/nlp/blob/master/sample/nlp-model_interpretation.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kth0fn9nnyBW",
        "colab_type": "text"
      },
      "source": [
        "## Fetch data\n",
        "\n",
        "We use the sklearn direct dataset 20 news groups."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBv_qGkzjcFv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kErEBZFLnJ8H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "train_raw_df = fetch_20newsgroups(subset='train')\n",
        "test_raw_df = fetch_20newsgroups(subset='test')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLKvmU9pnSGL",
        "colab_type": "code",
        "outputId": "9e44ade2-0049-43e3-e489-1f1249e21391",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "print(f'Number of raw training examples: {len(train_raw_df.data)}')\n",
        "print(f'Number of raw test examples: {len(test_raw_df.data)}')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of raw training examples: 11314\n",
            "Number of raw test examples: 7532\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tcsI_yLoSio",
        "colab_type": "code",
        "outputId": "3cdefcc2-6527-47e6-ee54-3df9eb31768f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "source": [
        "category_names = np.unique(np.array(train_raw_df.target_names))\n",
        "print(f'Number of different categories : {len(category_names)}')\n",
        "print(f'Category list: {category_names}')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of different categories : 20\n",
            "Category list: ['alt.atheism' 'comp.graphics' 'comp.os.ms-windows.misc'\n",
            " 'comp.sys.ibm.pc.hardware' 'comp.sys.mac.hardware' 'comp.windows.x'\n",
            " 'misc.forsale' 'rec.autos' 'rec.motorcycles' 'rec.sport.baseball'\n",
            " 'rec.sport.hockey' 'sci.crypt' 'sci.electronics' 'sci.med' 'sci.space'\n",
            " 'soc.religion.christian' 'talk.politics.guns' 'talk.politics.mideast'\n",
            " 'talk.politics.misc' 'talk.religion.misc']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYQ_WAHane2l",
        "colab_type": "code",
        "outputId": "96e2c666-616e-4931-a81e-199238c3c59c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        }
      },
      "source": [
        "print('Example of entry:')\n",
        "print(f'\\t - LABEL : {train_raw_df.target[0]} - {train_raw_df.target_names[0]}')\n",
        "print(f'\\t - {train_raw_df.data[0]}')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Example of entry:\n",
            "\t - LABEL : 7 - alt.atheism\n",
            "\t - From: lerxst@wam.umd.edu (where's my thing)\n",
            "Subject: WHAT car is this!?\n",
            "Nntp-Posting-Host: rac3.wam.umd.edu\n",
            "Organization: University of Maryland, College Park\n",
            "Lines: 15\n",
            "\n",
            " I was wondering if anyone out there could enlighten me on this car I saw\n",
            "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
            "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
            "the front bumper was separate from the rest of the body. This is \n",
            "all I know. If anyone can tellme a model name, engine specs, years\n",
            "of production, where this car is made, history, or whatever info you\n",
            "have on this funky looking car, please e-mail.\n",
            "\n",
            "Thanks,\n",
            "- IL\n",
            "   ---- brought to you by your neighborhood Lerxst ----\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfRxZq_HpKrK",
        "colab_type": "text"
      },
      "source": [
        "## Prepare data to the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZ_wiaxHoFbg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = train_raw_df.data\n",
        "y_train = train_raw_df.target\n",
        "\n",
        "x_test = test_raw_df.data\n",
        "y_test = test_raw_df.target"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_xK8v50qMq_",
        "colab_type": "text"
      },
      "source": [
        "## Model training\n",
        "\n",
        "We are going to use a model that doesn't use raw text as input. It use a vectorization of the text. For this, we are going to create a pipeline that allows combine the vectorizer and the model in the same structure.\n",
        "\n",
        "The vectorization is via TFIDF codification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43t_3fxkjlcS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Word Embedding via TFIDF - vectorization of the text\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Pipeline execution to combine vectorization and model execution\n",
        "from sklearn.pipeline import make_pipeline \n",
        "\n",
        "# Definition of generic models\n",
        "from sklearn.linear_model import LogisticRegression  # Logistic Regression"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xp2uU1qdjqY2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "8a4a2cc2-60f8-4cf7-a6f1-303af9870cac"
      },
      "source": [
        "# Train the vectorizer with the training documents to codify the text in a TFIDF representation \n",
        "vec = TfidfVectorizer()\n",
        "vec.fit(x_train)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
              "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
              "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
              "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
              "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
              "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                tokenizer=None, use_idf=True, vocabulary=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AxuWNrZjyIA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the model\n",
        "estimator = LogisticRegression(solver='newton-cg', n_jobs=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-Jh-PBMxSsX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the pipeline with the vectorizer and the model\n",
        "pipeline = make_pipeline(vec, estimator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhMA8xkKxU5x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "outputId": "1ffcc262-c489-46d8-c277-4f178d7990d6"
      },
      "source": [
        "# Train in the model\n",
        "pipeline.fit(x_train, y_train)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('tfidfvectorizer',\n",
              "                 TfidfVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.float64'>,\n",
              "                                 encoding='utf-8', input='content',\n",
              "                                 lowercase=True, max_df=1.0, max_features=None,\n",
              "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
              "                                 preprocessor=None, smooth_idf=True,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 sublinear_tf=False,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=None, use_idf=True,\n",
              "                                 vocabulary=None)),\n",
              "                ('logisticregression',\n",
              "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
              "                                    fit_intercept=True, intercept_scaling=1,\n",
              "                                    l1_ratio=None, max_iter=100,\n",
              "                                    multi_class='warn', n_jobs=-1, penalty='l2',\n",
              "                                    random_state=None, solver='newton-cg',\n",
              "                                    tol=0.0001, verbose=0, warm_start=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwP-RiEswyxZ",
        "colab_type": "text"
      },
      "source": [
        "## Model evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nkg1ypWjkOd7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "outputId": "14b2bdb8-eba4-4d42-9d7d-f4f5bdc2b2c8"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "predictions = pipeline.predict(x_test)\n",
        "\n",
        "print(classification_report(y_test,predictions))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.74      0.77       319\n",
            "           1       0.69      0.78      0.74       389\n",
            "           2       0.76      0.75      0.75       394\n",
            "           3       0.73      0.72      0.72       392\n",
            "           4       0.81      0.83      0.82       385\n",
            "           5       0.83      0.74      0.78       395\n",
            "           6       0.76      0.90      0.83       390\n",
            "           7       0.91      0.89      0.90       396\n",
            "           8       0.93      0.95      0.94       398\n",
            "           9       0.87      0.93      0.90       397\n",
            "          10       0.94      0.95      0.95       399\n",
            "          11       0.93      0.89      0.91       396\n",
            "          12       0.76      0.78      0.77       393\n",
            "          13       0.89      0.84      0.86       396\n",
            "          14       0.89      0.92      0.91       394\n",
            "          15       0.79      0.93      0.85       398\n",
            "          16       0.71      0.89      0.79       364\n",
            "          17       0.96      0.89      0.92       376\n",
            "          18       0.79      0.58      0.67       310\n",
            "          19       0.84      0.45      0.59       251\n",
            "\n",
            "    accuracy                           0.83      7532\n",
            "   macro avg       0.83      0.82      0.82      7532\n",
            "weighted avg       0.83      0.83      0.83      7532\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
